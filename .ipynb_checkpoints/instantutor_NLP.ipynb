{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0470c3-2781-40ee-8a12-c991403be1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import inflect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fff570b-7b57-4c83-a705-64aae28ce056",
   "metadata": {},
   "source": [
    "Obtain text from a user's request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10bc66c9-251e-486e-a21f-9ee04b6f8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is sample text. Will be used as an intermediate process during the running of the recommender model.\n",
    "sample_text = \"How do I find the determinant of a 3x3 matrix?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45118ea-2c13-46ea-bee2-9ebfabf8075f",
   "metadata": {},
   "source": [
    "Preprocess the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a10bb2-f80f-46d0-bcf8-1303e2eac27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how do i find the determinant of a 3x3 matrix?\n"
     ]
    }
   ],
   "source": [
    "#First we standardize sentences by removing punctuation, uniforming white spaces and lowercasing all words.\n",
    "\n",
    "s_words = sample_text.lower().split()\n",
    "strd_text = \" \".join(s_words)\n",
    "print(strd_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a84a32-abf8-4db7-81ed-157b8c67e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how do i find the determinant of a 3x3 matrix\n"
     ]
    }
   ],
   "source": [
    "#Next, we remove all punctuation\n",
    "strd_text = strd_text.translate(str.maketrans('','', string.punctuation))\n",
    "print(strd_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ca74b-3de7-4a6e-b8b9-6049c601118c",
   "metadata": {},
   "source": [
    "Optional: We can convert numbers to words and weigh them so that they will lean the sentence slightly towards being interpreted as math related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ef10cf-51c0-4c8d-b488-83117c2af161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how do i find the determinant of a 3x3 matrix\n"
     ]
    }
   ],
   "source": [
    "#Convert all numbers to words (ex: 3 -> three)\n",
    "p = inflect.engine()\n",
    "\n",
    "#split the words to parse for numbers, create a new array to store words\n",
    "old_string = strd_text.split()\n",
    "new_string = []\n",
    "\n",
    "for word in old_string:\n",
    "    if word.isdigit():\n",
    "        converted = p.number_to_words(word)\n",
    "        new_string.append(converted)\n",
    "    else:\n",
    "        new_string.append(word)\n",
    "        \n",
    "strd_text = \" \".join(new_string)\n",
    "\n",
    "print(strd_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3490a078-c6d8-459e-95f6-68241bd048c0",
   "metadata": {},
   "source": [
    "Clearly this did not work when we identified the dimensions of a matrix. This will need to be fixed in the future. Arguably, we could have treated that as a meaningless word and removed it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df7277-68c3-4037-8854-9ec7707f6b6a",
   "metadata": {},
   "source": [
    "Next, we want to remove any filler words that do not provide much meaning to a sentence outside of grammatical purposes. These words are known as stop words, such as \"a\" or \"the\". We will likely not need any question related words such as how, why, where as well, since the request should imply a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eaeedc7-3eea-4a23-9ebb-ff1e5c1bb8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['find', 'determinant', '3x3', 'matrix']\n"
     ]
    }
   ],
   "source": [
    "#remove stop words from the sentence.(We need to define a list of stop words, but we can simply obtain them from nltk.\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "word_tokens = word_tokenize(strd_text)\n",
    "strd_text = [word for word in word_tokens if word not in stop_words]\n",
    "print(strd_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f859468-8d50-40de-b555-1b57d5b7ae8c",
   "metadata": {},
   "source": [
    "Optional: \n",
    "Next, we will use stemming to obtain the root form of each \"meaningful\" word. Stemming may not give actual words, but it is much faster/more computationally efficient than lemmatizing, which guarantees actual words. It is also not mandatory, although it does reduce the size of necessary language dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13ff1b3d-8b12-46e4-95d9-faa4a7e51450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['find', 'determin', '3x3', 'matrix']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(word) for word in strd_text]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf18ae-2cff-435a-a826-16b5500116be",
   "metadata": {},
   "source": [
    "Now that we have preprocessed the sentence, we must vectorize it so that machine learning can be done on the sentence. For this process we will use word embeddings. The word2vec word embedding method will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74324f0-bac7-49a1-b649-b3423d495e64",
   "metadata": {},
   "source": [
    "For the word2vec method, we first need to determine a window size, for now, let this be win_size = 5. This means for each target word, we will take two words from both the left and right side to be the context. For sequential words not separated by spaces, we may use offsets but this is not necessary since we have tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e16dd-0d72-48f4-ac7e-e7b29a3beab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all context words for each target in a sentence. \n",
    "#For the sake of scalability, we will use indicies to represent words and have a mapping of words to indexes and the reverse\n",
    "#Normally while training, we are given a large amount of text to train on, so this process may take up to hours.\n",
    "'\n",
    "win_size = 5\n",
    "\n",
    "word_to_ind = {}\n",
    "ind_to_word = []\n",
    "\n",
    "#first count the frequency of each word in the sentence\n",
    "word_freq = {}\n",
    "\n",
    "for word in strd_text:\n",
    "    word_freq[word] = word_freq.get(word,0) + 1\n",
    "        \n",
    "text_len = len(strd_text)\n",
    "#then map each word to an index and each index to a word\n",
    "o = 0\n",
    "for word in word_freq.keys():\n",
    "    ind_to_word.push_back(word)\n",
    "    word_to_ind[word] = o\n",
    "    o += 1\n",
    "\n",
    "half_win = win_size//2\n",
    "\n",
    "context = [[] for word in ind_to_word]\n",
    "\n",
    "#we use np.clip so that we do not go out of bounds\n",
    "for i in range(text_len):\n",
    "    #treat each word as a target\n",
    "    targ_word = strd_text[i]\n",
    "    \n",
    "    for j in range(np.clip(i-half_win, 0, i), i):\n",
    "        context[targ_word].append(word_to_ind[strd_text[j]])\n",
    "    for k in range(i, np.clip(i+half_win, i, text_len)):\n",
    "        context[targ_word].append(word_to_ind[strd_text[k]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac850372-1cd7-4d3f-82f6-5ebdfecad888",
   "metadata": {},
   "source": [
    "Define a custom collate function to pad zeroes since our words may have different amounts of context and therefore, different number of pairs.\n",
    "The collate function simply stacks the get_item values for each batch item. This will be provided to our dataloader because the default collate function cannot handle tensors of different lengths, which is very common with sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7aa44-1806-42cf-8f51-43281636a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(batch):\n",
    "    \n",
    "    len1 = [len(item[1]) for item in batch]\n",
    "    dsum = [item[3] for item in batch]\n",
    "    \n",
    "    d = np.sum(dsum)\n",
    "    maxlen = max(len1)\n",
    "    \n",
    "    c_o = batch[0][2]\n",
    "    \n",
    "    t_o = torch.zeros(maxlen)\n",
    "    w_o = torch.zeros(maxlen)\n",
    "    \n",
    "    for item in batch:\n",
    "        c = item[2]\n",
    "        c_len = c.size(dim=0)\n",
    "        t_out = torch.zeros(maxlen)\n",
    "        w_out = torch.zeros(maxlen)\n",
    "        t_out[:c_len] = item[0]\n",
    "        w_out[:c_len] = item[1]\n",
    "        #torch.vstack((c_o, c))\n",
    "        torch.at_least_2d(t_out)\n",
    "        torch.at_least_2d(w_out)\n",
    "        torch.vstack((t_o, t_out.long()), axis = 1)\n",
    "        torch.vstack((w_o, w_out.long()), axis = 1)\n",
    "    \n",
    "    return t_o.long(), w_o.long(), c_o.long(), d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b761afd4-ce25-4fc5-916c-210dd221fa42",
   "metadata": {},
   "source": [
    "Define our word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d74cff-3a17-407f-af61-34e7c968872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(word2vec, self).__init__()\n",
    "        \n",
    "        #define the two layers\n",
    "        self.Target = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.Context = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, targets, contexts, len1):\n",
    "        #look up embeddings for the given targets and contexts\n",
    "        target_w = self.t(targets)\n",
    "        context_w = self.c(contexts)\n",
    "        \n",
    "        #find the dot product between corresponding pairs\n",
    "        dp = torch.sum(torch.mul(target_w, context_w), dim = 1)\n",
    "        \n",
    "        return dp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ddd7d8-7ab5-4e9b-8609-89f3fa9800c9",
   "metadata": {},
   "source": [
    "Define our training parameters, the batch_size, the learning rate, the number of epochs, and the embedding dimensions. For now, we will let embedding dim be 2 and batch size be 1 since our dataset is tiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d22c8b-829b-4646-bc85-86ad1307fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training parameters\n",
    "epochs = 3\n",
    "lr = 0.1\n",
    "batch_size = 1\n",
    "embedding_dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505617f-4ca9-403a-ae6d-c304ef392fc8",
   "metadata": {},
   "source": [
    "Define our dataset model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d49d2-d3a8-424f-ba1d-72793c8844b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentence_Dataset(Dataset):\n",
    "    def __init__(self, w_2in, in2_w, context, prob_dist, q):       \n",
    "        super(sentence_Dataset, self).__init__()\n",
    "        \n",
    "        self.w_2in = w_2in\n",
    "        self.in_2w = in_2w\n",
    "        self.context = context\n",
    "        self.prob_dist = prob_dist\n",
    "        self.q = q\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.in_w)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #to be defined\n",
    "        \n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
